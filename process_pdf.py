import json
import os
import re
import shutil
import sys
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import fitz  # PyMuPDF
import requests


ROOT = Path(__file__).resolve().parent
PDFS_TO_PROCESS_DIR = ROOT / "pdfs_to_process"
PUBLISHED_PDFS_DIR = ROOT / "pdfs"
POSTS_DIR = ROOT / "posts"
DATA_DIR = ROOT / "data"
ARTICLES_JSON_PATH = DATA_DIR / "articles.json"
PROCESSED_LEDGER_PATH = DATA_DIR / "processed_pdfs.json"
INDEX_HTML_PATH = ROOT / "index.html"
ARCHIVE_HTML_PATH = ROOT / "archive.html"


DEEPSEEK_CHAT_COMPLETIONS_URL = "https://api.deepseek.com/chat/completions"
DEEPSEEK_MODEL = "deepseek-chat"


@dataclass
class ExtractedPost:
    title: str
    summary: str
    expert_commentary: str


def ensure_dirs() -> None:
    PDFS_TO_PROCESS_DIR.mkdir(parents=True, exist_ok=True)
    POSTS_DIR.mkdir(parents=True, exist_ok=True)
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    PUBLISHED_PDFS_DIR.mkdir(parents=True, exist_ok=True)


def ensure_archive_html_exists() -> None:
    """
    å¦‚æœ archive.html ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªä¸é¦–é¡µé£æ ¼ä¸€è‡´ã€ç”¨äºå±•ç¤ºå…¨éƒ¨å†å²æŠ¥å‘Šçš„é¡µé¢ã€‚
    é¡µé¢åŒæ ·ä¾èµ– data/articles.json ä¸ script.js è¿›è¡Œæ¸²æŸ“ã€‚
    """
    html = """<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate">
    <meta http-equiv="Pragma" content="no-cache">
    <meta http-equiv="Expires" content="0">
    
    <title>è¡Œç ”ç²¾é€‰ Â· å¾€æœŸæŠ¥å‘Šå½’æ¡£ | Lili's Supply Chain AI Lab</title>
    <link rel="stylesheet" href="style.css?v=20240321">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ğŸ“Š</text></svg>">
</head>
<body>
    <div class="container">
        <header>
            <h1>ğŸ“ˆ è¡Œç ”ç²¾é€‰ Â· å¾€æœŸæŠ¥å‘Š</h1>
            <p class="subtitle">Lili's Supply Chain AI Lab Â· å†å²æŠ¥å‘Šå½’æ¡£</p>
            <div class="header-info">
                <span id="current-date"></span>
                <button onclick="window.location.href='index.html'">è¿”å›é¦–é¡µ</button>
            </div>
        </header>

        <main>
            <section class="intro">
                <h2>ğŸ“š å…¨éƒ¨å†å²æŠ¥å‘Š</h2>
                <p>è¿™é‡Œå±•ç¤ºæ‰€æœ‰å·²å‘å¸ƒçš„ç ”ç©¶æŠ¥å‘Šï¼ŒæŒ‰æ—¶é—´å€’åºæ’åˆ—ã€‚å»ºè®®ç»“åˆè¡Œä¸šèŠ‚å¥ã€æŠ€æœ¯å˜è¿ä¸ä¾›åº”é“¾å˜åŒ–è¿›è¡Œçºµå‘å¯¹æ¯”é˜…è¯»ã€‚</p>
            </section>

            <div id="articles-container">
                <p>æ­£åœ¨åŠ è½½å†å²æŠ¥å‘Š...</p>
            </div>
        </main>

        <footer>
            <p>Â© <span id="current-year"></span> Lili's Supply Chain AI Lab Â· Research & Insights</p>
            <p class="disclaimer">å†…å®¹ä»…ä¾›å­¦ä¹ ä¸ç ”ç©¶å‚è€ƒï¼Œä¸æ„æˆä»»ä½•æŠ•èµ„ã€æ³•å¾‹æˆ–åˆè§„å»ºè®®ã€‚ä¸šåŠ¡å†³ç­–å‰è¯·ç»“åˆè‡ªèº«æƒ…å†µå®¡æ…è¯„ä¼°ã€‚</p>
        </footer>
    </div>

    <script src="script.js?v=20240321"></script>
</body>
</html>
"""
    ARCHIVE_HTML_PATH.write_text(html, encoding="utf-8")


def read_pdf_text(pdf_path: Path, max_pages: int = 10, max_chars: int = 60_000) -> str:
    doc = fitz.open(pdf_path)
    parts: List[str] = []
    try:
        pages = min(len(doc), max_pages)
        for i in range(pages):
            text = doc.load_page(i).get_text("text")
            if text:
                parts.append(text)
            if sum(len(p) for p in parts) >= max_chars:
                break
    finally:
        doc.close()
    combined = "\n".join(parts).strip()
    if len(combined) > max_chars:
        combined = combined[:max_chars]
    return combined


def deepseek_extract_json(pdf_text: str, *, api_key: str) -> ExtractedPost:
    if not api_key:
        raise RuntimeError("ç¼ºå°‘ç¯å¢ƒå˜é‡ DEEPSEEK_API_KEY")

    system = (
        "ä½ æ˜¯ä¸€åæ·±åº¦ç†è§£ä¾›åº”é“¾ç®¡ç†ã€ç‰©æµæŠ€æœ¯ã€å…¨çƒè´¸æ˜“åˆè§„ä»¥åŠ AI åœ¨ä¾›åº”é“¾åº”ç”¨çš„èµ„æ·±ä¾›åº”é“¾é¡¾é—®ã€‚"
        "ä½ å°†ä»ç”¨æˆ·æä¾›çš„ PDF æ–‡æœ¬ä¸­æå–å…³é”®ä¿¡æ¯ï¼Œå¹¶é¢å‘ä¾›åº”é“¾ä»ä¸šè€…ç»™å‡ºå…·æœ‰å¯æ“ä½œæ€§çš„ä¸“ä¸šç‚¹è¯„ï¼Œ"
        "ç‰¹åˆ«å…³æ³¨å¯¹ä¾›åº”é“¾è§„åˆ’ã€é‡‡è´­ç­–ç•¥ã€åº“å­˜ä¸äº§èƒ½å¸ƒå±€ã€ç‰©æµç½‘ç»œè®¾è®¡ã€é£é™©ç®¡ç†å’Œåˆè§„è¦æ±‚çš„å½±å“ã€‚"
        "å¿…é¡»ä»…è¾“å‡º JSONï¼ˆä¸è¦è¾“å‡ºå¤šä½™æ–‡å­—ï¼‰ï¼Œå¹¶ç¡®ä¿å­—æ®µé½å…¨ã€‚"
    )

    user = (
        "è¯·ä»ä¸‹é¢çš„ PDF æ–‡æœ¬ä¸­æå–ä¿¡æ¯ï¼Œå¹¶ä¸¥æ ¼ä»¥ JSON å¯¹è±¡è¾“å‡ºã€‚\n\n"
        "è¦æ±‚ï¼š\n"
        "1) titleï¼šæŠ¥å‘Š/æ–‡ç« æ ‡é¢˜ï¼ˆä¸­æ–‡ä¼˜å…ˆï¼Œå°½é‡å®Œæ•´ï¼‰\n"
        "2) summaryï¼šæ ¸å¿ƒæ‘˜è¦ï¼ˆ5-10 æ¡è¦ç‚¹ï¼Œå¿…é¡»ä¸¥æ ¼éµå®ˆä»¥ä¸‹æ ¼å¼ï¼‰ï¼š\n"
        "   æ¯æ¡è¦ç‚¹æ ¼å¼ï¼šã€**æ€»ç»“è¯/çŸ­å¥**ã€‘ï¼šç´§æ¥ç€å±•å¼€ 1-2 å¥å…·ä½“çš„ç»†èŠ‚æè¿°ã€‚\n"
        "   ç¤ºä¾‹ï¼š\n"
        "   ã€**ç‰©æµé™æœ¬**ã€‘ï¼šé€šè¿‡å¼•å…¥ AI è·¯å¾„è§„åˆ’ç®—æ³•ï¼Œé¢„è®¡å¯é™ä½ 15% çš„æœ«ç«¯é…é€æˆæœ¬ã€‚\n"
        "   ã€**åˆè§„é£é™©**ã€‘ï¼šé’ˆå¯¹ 2026 å¹´æ–°çš„è´¸æ˜“æ³•æ¡ˆï¼ŒæŠ¥å‘Šæç¤ºäº†ç”µå­åŸä»¶è¿›å£çš„å‡†å…¥é™åˆ¶ã€‚\n"
        "   æ³¨æ„ï¼šæ€»ç»“è¯å¿…é¡»ç”¨ **åŠ ç²—æ ‡è®°** åŒ…è£¹ï¼Œæ¯æ¡è¦ç‚¹ç‹¬ç«‹ä¸€è¡Œï¼Œç”¨æ¢è¡Œåˆ†éš”ã€‚\n"
        "3) expert_commentaryï¼šä¸“å®¶ç‚¹è¯„ï¼ˆèµ„æ·±ä¾›åº”é“¾é¡¾é—®è§†è§’ï¼Œèšç„¦ä¾›åº”é“¾ç®¡ç†ã€ç‰©æµæŠ€æœ¯ã€è´¸æ˜“åˆè§„æˆ– AI/æ•°å­—åŒ–åœ¨ä¾›åº”é“¾ä¸­çš„åº”ç”¨ï¼Œ"
        "ç»“åˆæŠ¥å‘Šç»“è®ºè¯´æ˜å¯¹è¡Œä¸šä»ä¸šè€…åœ¨å†³ç­–ã€è¿è¥ä¼˜åŒ–å’Œé£é™©ç®¡ç†ä¸Šçš„å…·ä½“å½±å“ï¼Œå¹¶ç»™å‡ºå¯æ‰§è¡Œå»ºè®®ï¼Œ300-600 å­—ï¼‰ï¼š\n"
        "   å¿…é¡»ä¸¥æ ¼éµå®ˆä»¥ä¸‹æ ¼å¼ï¼š\n"
        "   æ¯æ¡æ´å¯Ÿæ ¼å¼ï¼šã€**æ€»ç»“è¯/çŸ­å¥**ã€‘ï¼šç´§æ¥ç€å±•å¼€ 1-2 å¥å…·ä½“çš„ç»†èŠ‚æè¿°ã€‚\n"
        "   ç¤ºä¾‹ï¼š\n"
        "   ã€**ç‰©æµé™æœ¬**ã€‘ï¼šé€šè¿‡å¼•å…¥ AI è·¯å¾„è§„åˆ’ç®—æ³•ï¼Œé¢„è®¡å¯é™ä½ 15% çš„æœ«ç«¯é…é€æˆæœ¬ã€‚\n"
        "   ã€**åˆè§„é£é™©**ã€‘ï¼šé’ˆå¯¹ 2026 å¹´æ–°çš„è´¸æ˜“æ³•æ¡ˆï¼ŒæŠ¥å‘Šæç¤ºäº†ç”µå­åŸä»¶è¿›å£çš„å‡†å…¥é™åˆ¶ã€‚\n"
        "   æ³¨æ„ï¼šæ€»ç»“è¯å¿…é¡»ç”¨ **åŠ ç²—æ ‡è®°** åŒ…è£¹ï¼Œæ¯æ¡æ´å¯Ÿç‹¬ç«‹ä¸€æ®µï¼Œç”¨æ¢è¡Œåˆ†éš”ã€‚\n\n"
        "è¾“å‡º JSON ç¤ºä¾‹ï¼š\n"
        "{\n"
        '  "title": "...",\n'
        '  "summary": "ã€**æ€»ç»“è¯1**ã€‘ï¼šæè¿°1\\nã€**æ€»ç»“è¯2**ã€‘ï¼šæè¿°2\\n...",\n'
        '  "expert_commentary": "ã€**æ´å¯Ÿ1**ã€‘ï¼šæè¿°1\\n\\nã€**æ´å¯Ÿ2**ã€‘ï¼šæè¿°2\\n\\n..."\n'
        "}\n\n"
        "PDF æ–‡æœ¬å¦‚ä¸‹ï¼ˆå¯èƒ½ä¸å®Œæ•´ï¼‰ï¼š\n"
        "-----\n"
        f"{pdf_text}\n"
        "-----\n"
    )

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }

    payload: Dict[str, Any] = {
        "model": DEEPSEEK_MODEL,
        "messages": [
            {"role": "system", "content": system},
            {"role": "user", "content": user},
        ],
        # å°½é‡å¼ºåˆ¶ JSON è¾“å‡ºï¼ˆDeepSeek æ”¯æŒ OpenAI å…¼å®¹å‚æ•°ï¼‰
        "response_format": {"type": "json_object"},
        "temperature": 0.2,
        "max_tokens": 1500,
    }

    resp = requests.post(DEEPSEEK_CHAT_COMPLETIONS_URL, headers=headers, json=payload, timeout=120)
    if resp.status_code != 200:
        raise RuntimeError(f"DeepSeek API è°ƒç”¨å¤±è´¥ï¼šHTTP {resp.status_code}\n{resp.text}")

    data = resp.json()
    content = (
        data.get("choices", [{}])[0]
        .get("message", {})
        .get("content", "")
        .strip()
    )
    if not content:
        raise RuntimeError(f"DeepSeek è¿”å›ä¸ºç©ºï¼š{json.dumps(data, ensure_ascii=False)[:2000]}")

    try:
        obj = json.loads(content)
    except json.JSONDecodeError:
        # å…œåº•ï¼šå°è¯•æˆªå–ç¬¬ä¸€ä¸ª JSON å¯¹è±¡
        m = re.search(r"\{[\s\S]*\}", content)
        if not m:
            raise
        obj = json.loads(m.group(0))

    title = str(obj.get("title", "")).strip()
    summary = str(obj.get("summary", "")).strip()
    expert = str(obj.get("expert_commentary", "")).strip()

    if not title or not summary or not expert:
        raise RuntimeError(f"DeepSeek JSON å­—æ®µç¼ºå¤±ï¼š{obj}")

    return ExtractedPost(title=title, summary=summary, expert_commentary=expert)


def slugify(text: str, max_len: int = 80) -> str:
    # å…è®¸ä¸­æ–‡ï¼šç”¨è¾ƒå®‰å…¨çš„æ–¹å¼ç”Ÿæˆ slugï¼ˆä¸­æ–‡ä¿ç•™ï¼Œç©ºç™½è½¬-ï¼Œç§»é™¤éæ³•æ–‡ä»¶åå­—ç¬¦ï¼‰
    t = text.strip()
    t = re.sub(r"\s+", "-", t)
    t = re.sub(r"[\\/:*?\"<>|]+", "", t)
    t = re.sub(r"-{2,}", "-", t).strip("-")
    if not t:
        t = "post"
    if len(t) > max_len:
        t = t[:max_len].rstrip("-")
    return t


def human_file_size(num_bytes: int) -> str:
    if num_bytes < 1024:
        return f"{num_bytes} B"
    for unit in ["KB", "MB", "GB", "TB"]:
        num_bytes /= 1024.0
        if num_bytes < 1024:
            return f"{num_bytes:.1f} {unit}"
    return f"{num_bytes:.1f} PB"


def load_json(path: Path, default: Any) -> Any:
    if not path.exists():
        return default
    return json.loads(path.read_text(encoding="utf-8"))


def save_json(path: Path, data: Any) -> None:
    path.write_text(json.dumps(data, ensure_ascii=False, indent=2) + "\n", encoding="utf-8")


def generate_post_html(post: ExtractedPost, *, post_title: str, date_str: str, pdf_rel_url: str) -> str:
    # å†…åµŒæ ·å¼ï¼šå³ä½¿å•é¡µä¹Ÿç¾è§‚
    safe_title = html_escape(post_title)
    # å¤„ç† Markdown åŠ ç²—æ ‡è®°ï¼Œç„¶åè½¬è¡Œ
    summary_html = nl2br(markdown_bold_to_html(post.summary))
    expert_html = nl2br(markdown_bold_to_html(post.expert_commentary))

    return f"""<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>{safe_title} | Lili's Supply Chain AI Lab</title>
  <style>
    :root {{
      --bg: #f5f7fb;
      --card: #ffffff;
      --text: #1f2d3d;
      --muted: #6b7a90;
      --primary: #2d7ff9;
      --border: #e8eef7;
    }}
    * {{ box-sizing: border-box; }}
    body {{
      margin: 0;
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "PingFang SC",
        "Hiragino Sans GB", "Microsoft YaHei", Arial, sans-serif;
      background: var(--bg);
      color: var(--text);
      line-height: 1.7;
    }}
    .wrap {{
      max-width: 960px;
      margin: 0 auto;
      padding: 28px 18px 60px;
    }}
    .topbar {{
      display: flex;
      gap: 10px;
      align-items: center;
      justify-content: space-between;
      margin-bottom: 16px;
    }}
    .crumb a {{
      color: var(--primary);
      text-decoration: none;
      font-weight: 600;
    }}
    .crumb a:hover {{ text-decoration: underline; }}
    .meta {{
      color: var(--muted);
      font-size: 14px;
    }}
    .card {{
      background: var(--card);
      border: 1px solid var(--border);
      border-radius: 14px;
      box-shadow: 0 10px 28px rgba(17, 38, 146, 0.08);
      padding: 22px 22px;
    }}
    h1 {{
      font-size: 28px;
      margin: 6px 0 8px;
      letter-spacing: 0.2px;
    }}
    .sub {{
      color: var(--muted);
      margin-bottom: 18px;
    }}
    .pill {{
      display: inline-block;
      padding: 6px 10px;
      border-radius: 999px;
      background: rgba(45, 127, 249, 0.10);
      color: var(--primary);
      font-weight: 700;
      font-size: 12px;
      margin-right: 8px;
    }}
    .section {{
      margin-top: 18px;
      padding-top: 18px;
      border-top: 1px dashed var(--border);
    }}
    .section h2 {{
      font-size: 18px;
      margin: 0 0 10px;
    }}
    .box {{
      background: #f8fbff;
      border: 1px solid #e7f0ff;
      border-radius: 12px;
      padding: 14px 14px;
      white-space: normal;
    }}
    .actions {{
      display: flex;
      flex-wrap: wrap;
      gap: 12px;
      margin-top: 18px;
    }}
    .btn {{
      display: inline-block;
      padding: 10px 14px;
      border-radius: 10px;
      text-decoration: none;
      font-weight: 700;
      border: 1px solid var(--border);
      background: #fff;
      color: var(--text);
    }}
    .btn.primary {{
      background: var(--primary);
      border-color: var(--primary);
      color: #fff;
    }}
    .btn:hover {{ transform: translateY(-1px); }}
    .footer {{
      margin-top: 18px;
      color: var(--muted);
      font-size: 13px;
      text-align: center;
    }}
  </style>
</head>
<body>
  <div class="wrap">
    <div class="topbar">
      <div class="crumb">
        <a href="../index.html">â† è¿”å›é¦–é¡µ</a>
      </div>
      <div class="meta">å‘å¸ƒäº {html_escape(date_str)}</div>
    </div>

    <div class="card">
      <div class="sub">
        <span class="pill">AI æ‘˜è¦</span>
        <span class="pill">ä¾›åº”é“¾è§†è§’</span>
      </div>
      <h1>{safe_title}</h1>
      <div class="sub">è‡ªåŠ¨ä» PDF æå–æ ‡é¢˜ã€æ ¸å¿ƒæ‘˜è¦ä¸ä¸“å®¶ç‚¹è¯„ã€‚</div>

      <div class="actions">
        <a class="btn primary" href="../{html_escape(pdf_rel_url)}" target="_blank" rel="noopener noreferrer">ä¸‹è½½åŸå§‹ PDF</a>
        <a class="btn" href="../index.html">æŸ¥çœ‹é¦–é¡µæ–‡ç« åˆ—è¡¨</a>
      </div>

      <div class="section">
        <h2>ğŸ¯ æ ¸å¿ƒæ‘˜è¦</h2>
        <div class="box">{summary_html}</div>
      </div>

      <div class="section">
        <h2>ğŸ’¬ ä¸“å®¶ç‚¹è¯„ï¼ˆä¾›åº”é“¾ä»ä¸šè€…è§†è§’ï¼‰</h2>
        <div class="box">{expert_html}</div>
      </div>

      <div class="footer">
        <div>Â© {datetime.now().year} Lili's Supply Chain AI Lab</div>
        <div>å…è´£å£°æ˜ï¼šå†…å®¹ä»…ä¾›å­¦ä¹ å‚è€ƒï¼Œä¸æ„æˆä»»ä½•æŠ•èµ„æˆ–å•†ä¸šå»ºè®®ã€‚</div>
      </div>
    </div>
  </div>
</body>
</html>
"""


def html_escape(s: str) -> str:
    return (
        s.replace("&", "&amp;")
        .replace("<", "&lt;")
        .replace(">", "&gt;")
        .replace('"', "&quot;")
        .replace("'", "&#39;")
    )


def nl2br(s: str) -> str:
    return "<br/>".join(s.splitlines())


def update_latest_post_block(index_html: str, *, title: str, url: str, date_str: str) -> str:
    start = "<!-- AUTO-GENERATED LATEST POST LINK START -->"
    end = "<!-- AUTO-GENERATED LATEST POST LINK END -->"
    if start not in index_html or end not in index_html:
        raise RuntimeError("index.html ä¸­æ‰¾ä¸åˆ° AUTO-GENERATED LATEST POST LINK æ ‡è®°å—")

    block = f"""<!-- AUTO-GENERATED LATEST POST LINK START -->
            <div id="latest-post" style="margin: 0 0 18px 0;">
                <div style="background:#f8fbff;border:1px solid #e7f0ff;border-radius:10px;padding:14px 16px;">
                    <div style="font-weight:700;color:#2c3e50;margin-bottom:6px;">ğŸ†• æœ€æ–°è§£è¯»ï¼ˆè‡ªåŠ¨å‘å¸ƒï¼‰</div>
                    <div style="color:#7f8c8d;font-size:0.95rem;margin-bottom:10px;">{html_escape(date_str)}</div>
                    <a href="{html_escape(url)}" style="color:#3498db;font-weight:700;text-decoration:none;">{html_escape(title)}</a>
                </div>
            </div>
            <!-- AUTO-GENERATED LATEST POST LINK END -->"""

    pattern = re.compile(re.escape(start) + r"[\s\S]*?" + re.escape(end))
    return pattern.sub(block, index_html, count=1)


def upsert_article_entry(
    articles: List[Dict[str, Any]],
    *,
    title: str,
    date_str: str,
    core_viewpoints_html: str,
    comments_html: str,
    pdf_url: str,
    file_size: str,
    post_url: str,
) -> List[Dict[str, Any]]:
    # æ–°æ–‡ç« æ”¾æœ€ä¸Šæ–¹ï¼›id è‡ªåŠ¨é€’å¢
    max_id = 0
    for a in articles:
        try:
            max_id = max(max_id, int(a.get("id", 0)))
        except Exception:
            continue

    entry: Dict[str, Any] = {
        "id": max_id + 1,
        "title": title,
        "date": date_str,
        "coreViewpoints": core_viewpoints_html,
        "comments": comments_html,
        "pdfUrl": pdf_url,
        "fileSize": file_size,
        "postUrl": post_url,
        "summary": strip_html(core_viewpoints_html)[:280],
    }

    # å»é‡ï¼šè‹¥å·²æœ‰åŒæ ‡é¢˜ä¸” pdfUrl ç›¸åŒï¼Œåˆ™ä¸é‡å¤æ’å…¥
    for existing in articles:
        if str(existing.get("title", "")).strip() == title and str(existing.get("pdfUrl", "")).strip() == pdf_url:
            return articles

    return [entry] + articles


def strip_html(s: str) -> str:
    return re.sub(r"<[^>]+>", "", s or "").strip()


def markdown_bold_to_html(text: str) -> str:
    """
    å°† Markdown æ ¼å¼çš„åŠ ç²— **æ–‡æœ¬** è½¬æ¢ä¸º HTML <strong>æ–‡æœ¬</strong>
    ç­–ç•¥ï¼šå…ˆæå–åŠ ç²—å†…å®¹å¹¶è½¬ä¹‰ï¼Œæ›¿æ¢ä¸ºå ä½ç¬¦ï¼Œè½¬ä¹‰æ•´è¡Œï¼Œå†æ›¿æ¢å ä½ç¬¦ä¸º <strong> æ ‡ç­¾
    """
    # ä½¿ç”¨å ä½ç¬¦é¿å…è½¬ä¹‰å†²çª
    placeholders = {}
    placeholder_counter = [0]
    
    def extract_bold(match):
        content = match.group(1)
        # å¯¹åŠ ç²—å†…å®¹è¿›è¡Œ HTML è½¬ä¹‰
        escaped_content = html_escape(content)
        placeholder = f"__BOLD_PLACEHOLDER_{placeholder_counter[0]}__"
        placeholder_counter[0] += 1
        placeholders[placeholder] = escaped_content
        return placeholder
    
    # åŒ¹é… **æ–‡æœ¬** æ¨¡å¼ï¼ˆéè´ªå©ªï¼Œé¿å…åŒ¹é…åµŒå¥—ï¼‰
    pattern = r'\*\*([^*]+?)\*\*'
    # å…ˆç”¨å ä½ç¬¦æ›¿æ¢æ‰€æœ‰åŠ ç²—æ ‡è®°
    text_with_placeholders = re.sub(pattern, extract_bold, text)
    
    # è½¬ä¹‰æ•´è¡Œçš„å…¶ä»–å†…å®¹
    escaped_text = html_escape(text_with_placeholders)
    
    # å°†å ä½ç¬¦æ›¿æ¢ä¸º <strong> æ ‡ç­¾
    result = escaped_text
    for placeholder, content in placeholders.items():
        result = result.replace(placeholder, f'<strong>{content}</strong>')
    
    return result


def render_summary_as_html_list(summary: str) -> str:
    # summary å¯èƒ½æ˜¯å¤šè¡Œæˆ–æ¡ç›®æ–‡æœ¬ï¼šåœ¨é¡µé¢ä¸­æ¸²æŸ“ä¸ºä¸€ç»„æ´å¯Ÿæ¡ç›®
    # å¤„ç† Markdown åŠ ç²—æ ‡è®°ï¼ˆ**æ–‡æœ¬** -> <strong>æ–‡æœ¬</strong>ï¼‰ï¼Œå¹¶ä¸ºæ¯æ¡å¤–å±‚åŒ…è£¹ .insight-item
    lines = [ln.strip() for ln in summary.splitlines() if ln.strip()]
    if not lines:
        # å³ä½¿åªæœ‰ä¸€è¡Œï¼Œä¹Ÿè¦å¤„ç†åŠ ç²—æ ‡è®°
        content = markdown_bold_to_html(summary)
        return f'<div class="insight-item">{content}</div>'
    
    # å¯¹æ¯ä¸€è¡Œå¤„ç†åŠ ç²—æ ‡è®°ï¼Œå¹¶åŒ…è£¹ä¸ºç‹¬ç«‹çš„æ´å¯Ÿæ¡ç›®
    processed_lines = [
        f'<div class="insight-item">{markdown_bold_to_html(ln)}</div>'
        for ln in lines
    ]
    return "".join(processed_lines)


def main() -> int:
    ensure_dirs()
    ensure_archive_html_exists()

    api_key = os.environ.get("DEEPSEEK_API_KEY", "").strip()

    # è¯»å– ledger
    processed: List[str] = load_json(PROCESSED_LEDGER_PATH, default=[])
    processed_set = set(processed)

    # ç¡®ä¿ articles.json å­˜åœ¨
    if ARTICLES_JSON_PATH.exists():
        articles: List[Dict[str, Any]] = load_json(ARTICLES_JSON_PATH, default=[])
    else:
        articles = []

    pdf_files = sorted(PDFS_TO_PROCESS_DIR.glob("*.pdf"))
    if not pdf_files:
        print("pdfs_to_process/ ä¸­æ²¡æœ‰å¾…å¤„ç† PDFã€‚")
        return 0

    any_changed = False

    for pdf_path in pdf_files:
        if pdf_path.name in processed_set:
            print(f"è·³è¿‡å·²å¤„ç†ï¼š{pdf_path.name}")
            continue

        print(f"å¼€å§‹å¤„ç†ï¼š{pdf_path.name}")

        pdf_text = read_pdf_text(pdf_path)
        if not pdf_text:
            print(f"è­¦å‘Šï¼šPDF æå–æ–‡æœ¬ä¸ºç©ºï¼Œè·³è¿‡ï¼š{pdf_path.name}")
            continue

        extracted = deepseek_extract_json(pdf_text, api_key=api_key)

        # å‘å¸ƒ PDFï¼šç§»åŠ¨åˆ° /pdfs
        target_pdf_path = PUBLISHED_PDFS_DIR / pdf_path.name
        if target_pdf_path.exists():
            # é˜²æ­¢è¦†ç›–ï¼šåŠ æ—¶é—´æˆ³
            stem = target_pdf_path.stem
            ts = datetime.now().strftime("%Y%m%d%H%M%S")
            target_pdf_path = PUBLISHED_PDFS_DIR / f"{stem}-{ts}{target_pdf_path.suffix}"

        shutil.move(str(pdf_path), str(target_pdf_path))
        pdf_rel_url = f"pdfs/{target_pdf_path.name}"

        # ç”Ÿæˆ post HTML
        date_str = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
        slug = slugify(extracted.title)
        post_filename = f"{datetime.now().strftime('%Y%m%d')}-{slug}.html"
        post_path = POSTS_DIR / post_filename
        post_rel_url = f"posts/{post_filename}"

        html = generate_post_html(extracted, post_title=extracted.title, date_str=date_str, pdf_rel_url=pdf_rel_url)
        post_path.write_text(html, encoding="utf-8")

        # æ›´æ–° articles.jsonï¼ˆç”¨äºé¦–é¡µæ¸²æŸ“ï¼‰
        file_size = human_file_size(target_pdf_path.stat().st_size)
        core_viewpoints_html = render_summary_as_html_list(extracted.summary)
        comments_html = render_summary_as_html_list(extracted.expert_commentary)
        articles = upsert_article_entry(
            articles,
            title=extracted.title,
            date_str=date_str,
            core_viewpoints_html=core_viewpoints_html,
            comments_html=comments_html,
            pdf_url=pdf_rel_url,
            file_size=file_size,
            post_url=post_rel_url,
        )

        # æ›´æ–° index.html æœ€æ–°é“¾æ¥å—
        index_html = INDEX_HTML_PATH.read_text(encoding="utf-8")
        index_html = update_latest_post_block(index_html, title=extracted.title, url=post_rel_url, date_str=date_str)
        INDEX_HTML_PATH.write_text(index_html, encoding="utf-8")

        processed.append(pdf_path.name)
        processed_set.add(pdf_path.name)

        any_changed = True
        print(f"å·²å‘å¸ƒï¼š{post_rel_url} ï¼ˆPDFï¼š{pdf_rel_url}ï¼‰")

    if any_changed:
        save_json(ARTICLES_JSON_PATH, articles)
        save_json(PROCESSED_LEDGER_PATH, processed)
        print("å¤„ç†å®Œæˆï¼šå·²æ›´æ–° index.html / data/articles.json / posts/ / pdfs/")
    else:
        print("æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–° PDFã€‚")

    return 0


if __name__ == "__main__":
    raise SystemExit(main())
